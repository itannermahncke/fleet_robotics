{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from m2bk import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_handler = DatasetHandler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show gray image\n",
    "image = dataset_handler.images[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rgb image\n",
    "image_rgb = dataset_handler.images_rgb[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show depth\n",
    "i = 0\n",
    "depth = dataset_handler.depth_maps[i]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(depth, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Depth map shape: {0}\".format(depth.shape))\n",
    "\n",
    "v, u = depth.shape\n",
    "depth_val = depth[v-1, u-1]\n",
    "print(\"Depth value of the very bottom-right pixel of depth map {0} is {1:0.3f}\".format(i, depth_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k - camera calibration matrix\n",
    "dataset_handler.k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Feature Extraction\n",
    "\n",
    "1.1 - Extracting Features from an Image\n",
    "\n",
    "__Task:__ Implement feature extraction from a single image. You can use any feature descriptor of your choice covered in the lectures, ORB for example.\n",
    "\n",
    "Note 1: Make sure you understand the structure of the keypoint descriptor object, this will be very useful for your further tasks. You might find [OpenCV: Keypoint Class Description](https://docs.opencv.org/3.4.3/d2/d29/classcv_1_1KeyPoint.html) handy. \n",
    "\n",
    "Note 2: Make sure you understand the image coordinate system, namely the origin location and axis directions.\n",
    "\n",
    "Note 3: We provide you with a function to visualise the features detected. Run the last 2 cells in section 1.1 to view.\n",
    "\n",
    "Optional: Try to extract features with different descriptors such as SIFT, ORB, SURF and BRIEF. You can also try using detectors such as Harris corners or FAST and pairing them with a descriptor. Lastly, try changing parameters of the algorithms. Do you see the difference in various approaches? You might find this link useful: OpenCV:Feature Detection and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Find keypoints and descriptors for the image\n",
    "\n",
    "    Arguments:\n",
    "    image -- a grayscale image\n",
    "\n",
    "    Returns:\n",
    "    kp -- list of the extracted keypoints (features) in an image\n",
    "    des -- list of the keypoint descriptors in an image\n",
    "    \"\"\"\n",
    "    # Create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    # Detect keypoints and compute descriptors\n",
    "    kp, des = sift.detectAndCompute(image, None)\n",
    "\n",
    "#     # Initiate ORB detector\n",
    "#     orb = cv.ORB_create()\n",
    "#     # find the keypoints and descriptors with ORB\n",
    "#     kp, des = orb.detectAndCompute(image,None)\n",
    "    \n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image = dataset_handler.images[i]\n",
    "kp, des = extract_features(image)\n",
    "print(kp[1].pt)\n",
    "print(\"Number of features detected in frame {0}: {1}\\n\".format(i, len(kp)))\n",
    "\n",
    "print(\"Coordinates of the first keypoint in frame {0}: {1}\".format(i, str(kp[0].pt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(image, kp):\n",
    "    \"\"\"\n",
    "    Visualize extracted features in the image\n",
    "\n",
    "    Arguments:\n",
    "    image -- a grayscale image\n",
    "    kp -- list of the extracted keypoints\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    display = cv2.drawKeypoints(image, kp, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: visualizing and experimenting with various feature descriptors\n",
    "i = 0\n",
    "image = dataset_handler.images_rgb[i]\n",
    "\n",
    "visualize_features(image, kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 - Extracting Features from Each Image in the Dataset\n",
    "Task: Implement feature extraction for each image in the dataset with the function you wrote in the above section.\n",
    "\n",
    "Note: If you do not remember how to pass functions as arguments, make sure to brush up on this topic. This Passing Functions as Arguments might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dataset(images, extract_features_function):\n",
    "    \"\"\"\n",
    "    Find keypoints and descriptors for each image in the dataset\n",
    "\n",
    "    Arguments:\n",
    "    images -- a list of grayscale images\n",
    "    extract_features_function -- a function which finds features (keypoints and descriptors) for an image\n",
    "\n",
    "    Returns:\n",
    "    kp_list -- a list of keypoints for each image in images\n",
    "    des_list -- a list of descriptors for each image in images\n",
    "    \n",
    "    \"\"\"\n",
    "    kp_list = []\n",
    "    des_list = []\n",
    "    \n",
    "    for img in images:\n",
    "        # extract keypoints and descriptors\n",
    "        kp, des = extract_features_function(img)\n",
    "        kp_list.append(kp)\n",
    "        des_list.append(des)\n",
    "    \n",
    "    return kp_list, des_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset_handler.images\n",
    "kp_list, des_list = extract_features_dataset(images, extract_features)\n",
    "\n",
    "i = 0\n",
    "print(\"Number of features detected in frame {0}: {1}\".format(i, len(kp_list[i])))\n",
    "print(\"Coordinates of the first keypoint in frame {0}: {1}\\n\".format(i, str(kp_list[i][0].pt)))\n",
    "\n",
    "# Remember that the length of the returned by dataset_handler lists should be the same as the length of the image array\n",
    "print(\"Length of images array: {0}\".format(len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Feature Matching\n",
    "\n",
    "Next step after extracting the features in each image is matching the features from the subsequent frames. This is what is needed to be done in this section.\n",
    "\n",
    "2.1 - Matching Features from a Pair of Subsequent Frames\n",
    "Task: Implement feature matching for a pair of images. You can use any feature matching algorithm of your choice covered in the lectures, Brute Force Matching or FLANN based Matching for example.\n",
    "\n",
    "Optional 1: Implement match filtering by thresholding the distance between the best matches. This might be useful for improving your overall trajectory estimation results. Recall that you have an option of specifying the number best matches to be returned by the matcher.\n",
    "\n",
    "We have provided a visualization of the found matches. Do all the matches look legitimate to you? Do you think match filtering can improve the situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
